{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding User Behavior\n",
    "### Sophia Skowronski | Project 3\n",
    "### Summer 2020 | MIDS w205 | Fundamentals of Data Engineering\n",
    "\n",
    "## Summary\n",
    "\n",
    "Our mobile game has two events we're interested in tracking: buy a sword & join guild. This report is a summary of how the data pipeline was set up so that our data anayltics team can use it to query and solve the business problems at hand.\n",
    "\n",
    "## Annotations\n",
    "\n",
    "### Summary of the web-app server and streaming files.\n",
    "\n",
    "#### Web-app server (`game_api.py`): Add `purchase_sword_event` and `join_guild_event`. Code snippet below:\n",
    "```\n",
    "    @app.route(\"/join_a_guild\")\n",
    "    def join_a_guild():\n",
    "        join_guild_event = {'event_type': 'join_guild'}\n",
    "        log_to_kafka('events', join_guild_event)\n",
    "        return \"Guild Joined!\\n\"\n",
    "```\n",
    "- In the `game_api.py` file, we import the Flask module and create a Flask web server.\n",
    "- Then, we create an instance of the Flask class and call it `app`.\n",
    "- There are a number of web applications: a `default` page, a `purchase_a_sword` page, and an additional `join_a_guild` page.\n",
    "- When the user goes to the `join_a_guild` page of the website, the function above will get activated, sending a dictionary with a simple metadata characterstic  (`event_type`) to the `log_to_kafka` function, which combines the request headers with the event data, and then, that message is sent (via the imported `KafKaPRoducer` module) as an encoded json to the `events` topic.\n",
    "\n",
    "#### Spark streaming file (`write_stream.py`): Create separate sinks for the two event types. Code snippet below:\n",
    "```\n",
    "    guild_joins = raw_events \\\n",
    "        .filter(is_join_guild_event(raw_events.value.cast('string'))) \\\n",
    "        .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "                raw_events.timestamp.cast('string'),\n",
    "                from_json(raw_events.value.cast('string'),\n",
    "                    event_schema()).alias('json')) \\\n",
    "        .select('raw_event', 'timestamp', 'json.*')\n",
    "    ...\n",
    "    guild_sink = guild_joins \\\n",
    "        .writeStream \\\n",
    "        .format(\"parquet\") \\\n",
    "        .option(\"checkpointLocation\", \"/tmp/checkpoints_for_guild_joins\") \\\n",
    "        .option(\"path\", \"/tmp/guild_joins\") \\\n",
    "        .trigger(processingTime=\"10 seconds\") \\\n",
    "        .start()\n",
    "    ...\n",
    "    spark.streams.awaitAnyTermination()\n",
    "```\n",
    "- In the code above, this is one example of a DataFrame object that is created, written to a sink stream, and stored in HDFS.\n",
    "- The `write_stream.py` file imports libraries, infers schema by describing what the data is supposed to look like (in a function called `event_schema`), calls `@udf` Boolean functions to filter the event data, and initializes a Spark session.\n",
    "- Within the Spark session, Sparks reads from the Kafka \"events\" stream, does filtered transformations on the source events by calling the `@udf` functions, creates two sinks for the data, and writes those sinks to Parquet. A trigger is defined so Spark processes the data and sends it to HDFS every 10 seconds. \n",
    "    - Note that there are no offsets defined, it will run continuously until it gets an end signal.\n",
    "- Because there are two streams running, the final line of code activates the StreamingQueryManager to manage the two active sinks. It blocks any shutdown until either one of them terminates.\n",
    "- At the bottom of the file, an if statement activates the `main()` function. `__name__` references the current file. Python assigns the name `__main__` to the script when executed. If we import another script, the if statement will prevent other scripts from running. When we run `write_stream.py`, it will change its name to `__main__`, and only then will that if statement activate.\n",
    "\n",
    "### Spin up the cluser.\n",
    "```\n",
    "docker-compose up -d\n",
    "```\n",
    "### Terminal 1: Run flask.\n",
    "```\n",
    "docker-compose exec mids env FLASK_APP=/w205/project-3-sophiaski/game_api.py flask run --host 0.0.0.0\n",
    "```\n",
    "- This runs the Flask server, adding `--host 0.0.0.0` to the end makes the server publicly available. It tells the operating system to listen on all public IPs.\n",
    "- The `FLASK_APP` environment variable is the name of the module to import at `flask run`.\n",
    "\n",
    "### Terminal 2: From the MIDS container, use kafkacat to print out the messages from the specified Kafka broker and \"events\" topic, dumping them into standard output.\n",
    "```\n",
    "docker-compose exec mids kafkacat -C -b kafka:29092 -t events -o beginning\n",
    "```\n",
    "- This command needs to be run twice since the topic does not exist, yet, but it will be created after it is called a second time..\n",
    "- Excluding `-e` from the command ensures it will run continuously.\n",
    "\n",
    "### Terminal 3: Check out HDFS to see if anything has been written to `/tmp/`.\n",
    "```\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/\n",
    "```\n",
    "- Nothing yet!\n",
    "\n",
    "### Terminal 3: Run the Spark stream.\n",
    "```\n",
    "docker-compose exec spark spark-submit /w205/project-3-sophiaski/write_stream.py\n",
    "```\n",
    "- Run `spark-submit` command from within the Spark container to launch the python application.\n",
    "\n",
    "### Terminal 4: Again, check out HDFS to see what has been written to `/tmp/`.\n",
    "```\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/\n",
    "```\n",
    "```\n",
    "drwxrwxrwt   - root   supergroup          0 2020-07-29 19:39 /tmp/checkpoints_for_guild_joins\n",
    "drwxrwxrwt   - root   supergroup          0 2020-07-29 19:39 /tmp/checkpoints_for_sword_purchases\n",
    "drwxr-xr-x   - root   supergroup          0 2020-07-29 19:39 /tmp/guild_joins\n",
    "drwxr-xr-x   - root   supergroup          0 2020-07-29 19:39 /tmp/sword_purchases\n",
    "```\n",
    "- There are new folders in HDFS. The streaming file adds locations for the streaming data sinks. \n",
    "- In the newly created folders, there are a snappy-compressed parquet files with spark metadata. Spark SQL caches Parquet metadata for better performance.\n",
    "- A check pointing method provides fault tolerance to the streaming data, saving those checkpoints as separate folders in HDFS.\n",
    "\n",
    "### Terminal 4: Set up Presto.\n",
    "```\n",
    "docker-compose exec cloudera hive\n",
    "hive> create external table if not exists default.sword_purchases (\n",
    "    raw_event string,\n",
    "    timestamp string,\n",
    "    Accept string,\n",
    "    Host string,\n",
    "    User_Agent string,\n",
    "    event_type string\n",
    "  )\n",
    "  stored as parquet \n",
    "  location '/tmp/sword_purchases'\n",
    "  tblproperties (\"parquet.compress\"=\"SNAPPY\");\n",
    "hive> create external table if not exists default.guild_joins (\n",
    "    raw_event string,\n",
    "    timestamp string,\n",
    "    Accept string,\n",
    "    Host string,\n",
    "    User_Agent string,\n",
    "    event_type string\n",
    "  )\n",
    "  stored as parquet \n",
    "  location '/tmp/guild_joins'\n",
    "  tblproperties (\"parquet.compress\"=\"SNAPPY\");\n",
    "```\n",
    "- This sets up two Hive external tables in the HDFS container via the Cloudera Hive console, which stores the schema, the data location, as well as table properties.\n",
    "\n",
    "### Terminal 4: Query HDFS with Presto \n",
    "```\n",
    "docker-compose exec presto presto --server presto:8080 --catalog hive --schema default\n",
    "```\n",
    "- Presto is talking to the Hive server to get the `sword_purchases` and `guild_joins` tables, and these are connected to HDFS to retrieve the data for queries.\n",
    "\n",
    "### Terminal 5: With the Spark stream running, kick in events to the web-app server via a shell script to automaically generate events.\n",
    "```\n",
    "#!/bin/sh \n",
    "\n",
    "while true\n",
    "\tdo\n",
    "\tdocker-compose exec mids ab -n $(($RANDOM%25+6)) -H \"Host: user1.comcast.com\" http://localhost:5000/\n",
    "\tdocker-compose exec mids ab -n $(($RANDOM%25+6)) -H \"Host: user1.comcast.com\" http://localhost:5000/purchase_a_sword\n",
    "\tdocker-compose exec mids ab -n $(($RANDOM%25+6)) -H \"Host: user1.comcast.com\" http://localhost:5000/join_a_guild\n",
    "\tdocker-compose exec mids ab -n $(($RANDOM%50+1)) -H \"Host: user2.att.com\" http://localhost:5000/\n",
    "\tdocker-compose exec mids ab -n $(($RANDOM%50+1)) -H \"Host: user2.att.com\" http://localhost:5000/purchase_a_sword\n",
    "\tdocker-compose exec mids ab -n $(($RANDOM%50+1)) -H \"Host: user2.att.com\" http://localhost:5000/join_a_guild\n",
    "\tsleep 3\n",
    "done\n",
    "```\n",
    "- This uses Apache Benche to benchmark the HTTP requests and simulate data by feeding in a random number of requests to perform and appending host information to the header of the request. \n",
    "\n",
    "### Terminal 4: Query HDFS with Presto after data has been loaded in.\n",
    "\n",
    "The events were split into two tables for scalability purposes. Keeping in mind the future development of the game, there will likely be many more event types that need to be tracked, each with their own event meta-data structure and schema that is not shared by other types of actions. It is also important for storage and security reasons, given that our anayltics team may or may not need access to the entire dataset to solve a specific business problem. See below for some example queries and findings that our data analytics team might find useful.\n",
    "```\n",
    "presto:default> show tables;\n",
    "      Table      \n",
    "-----------------\n",
    " guild_joins     \n",
    " sword_purchases \n",
    "(2 rows)\n",
    "\n",
    "Query 20200802_224911_00003_x7r64, FINISHED, 1 node\n",
    "Splits: 2 total, 0 done (0.00%)\n",
    "0:00 [0 rows, 0B] [0 rows/s, 0B/s]\n",
    "```\n",
    "\n",
    "## Basic Analytics Questions & Queries\n",
    "\n",
    "### How many total events were tracked during the session?\n",
    "```\n",
    "presto:default> select count(*) as total_events from (select * from guild_joins union select * from sword_purchases as guild_and_swords);\n",
    " total_events \n",
    "--------------\n",
    "         1278 \n",
    "(1 row)\n",
    "\n",
    "Query 20200802_224939_00004_x7r64, FINISHED, 1 node\n",
    "Splits: 147 total, 137 done (93.20%)\n",
    "0:14 [1.04K rows, 130KB] [74 rows/s, 9.34KB/s]\n",
    "```\n",
    "- During this session, there were 1278 events. This query joined the two tables and counted the total number of non-default events (e.g. sword purchase or guild joins) tracked so far in the current session.\n",
    "\n",
    "### Which host had a higher number of sword purchases?\n",
    "```\n",
    "presto:default> select host, count(*) as total_events from sword_purchases group by host;\n",
    "       host        | total_events \n",
    "-------------------+--------------\n",
    " user1.comcast.com |          223 \n",
    " user2.att.com     |          302 \n",
    "(2 rows)\n",
    "\n",
    "Query 20200802_230759_00010_azxik, FINISHED, 1 node\n",
    "Splits: 94 total, 89 done (94.68%)\n",
    "0:03 [523 rows, 75.3KB] [197 rows/s, 28.4KB/s]\n",
    "```\n",
    "- Querying from the `sword_purchases` table, the AT&T host had a higher number of sword purchases than the Comcast host.\n",
    "\n",
    "### Which host had the higher number of guilds joined?\n",
    "```\n",
    "presto:default> select host, count(*) as total_events from guild_joins group by host;\n",
    "       host        | total_events \n",
    "-------------------+--------------\n",
    " user2.att.com     |          465 \n",
    " user1.comcast.com |          288 \n",
    "(2 rows)\n",
    "\n",
    "Query 20200802_230858_00011_azxik, FINISHED, 1 node\n",
    "Splits: 96 total, 87 done (90.63%)\n",
    "0:03 [747 rows, 82KB] [286 rows/s, 31.4KB/s]\n",
    "```\n",
    "- Querying from the `guild_joins` table, the AT&T host had a higher number of guild joins than the Comcast host.\n",
    "\n",
    "### Were more swords purchased than guilds joined?\n",
    "```\n",
    "presto:default> select event_type, count(*) as total_events from (select * from guild_joins union select * from sword_purchases as guild_and_swords) group by event_type;\n",
    "   event_type   | total_events \n",
    "----------------+--------------\n",
    " join_guild     |          753 \n",
    " purchase_sword |          525 \n",
    "(2 rows)\n",
    "\n",
    "Query 20200802_231014_00013_azxik, FINISHED, 1 node\n",
    "Splits: 191 total, 178 done (93.19%)\n",
    "0:05 [1.21K rows, 155KB] [243 rows/s, 31.2KB/s]\n",
    "```\n",
    "- Querying from the joined `sword_purchases` and `guild_joins` tables, we can group by `event-type` to retrieve the result of this query. In this session, the majority of events were from either host joining a guild, with 753 total events.\n",
    "\n",
    "### Spin down the cluser.\n",
    "```\n",
    "docker-compose down\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
