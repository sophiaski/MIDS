{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tracking User Activity on EdTech Assessments**\n",
    "### Sophia Skowronski | Project 2\n",
    "### Summer 2020 | MIDS w205 | Fundamentals of Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "An EdTech firm created a tool that delivers data from assessments on online learning, and it has been prepared here for customers to run additional queries for publishing their results. The annotations below provide a step-by-step overview of how the data was written to, streamed, and consumed via Kafka, transformed via Spark, and stored via Hadoop Distributed File System (HDFS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotations\n",
    "\n",
    "#### Open the directory\n",
    "`cd w205/project-2-sophiaski/`\n",
    "\n",
    "#### Copy the docker-compose file that has Spark stack with Kafka and HDFS\n",
    "`cp ~/w205/course-content/08-Querying-Data/docker-compose.yml .`\n",
    "- I added open ports to `docker-compose.yml` to allow interaction with spark service, using a custom port 8850 for Jupyter Notebook in order to not conflict with the default on the Google Cloud Platform instance at 8888.\n",
    "\n",
    "#### Download the assessments dataset\n",
    "`curl -L -o assessment-attempts-20180128-121051-nested.json https://goo.gl/ME6hjp`\n",
    "\n",
    "#### Check that the docker compose file and json file are in the directory \n",
    "`ls`\n",
    "\n",
    "#### Spin up the cluster in detached mode, running the containers in the background\n",
    "`docker-compose up -d`\n",
    "\n",
    "#### Create a topic named \"assess\" with a single partition and only one replica\n",
    "`docker-compose exec kafka kafka-topics --create --topic assess --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181`\n",
    "- There is only one partition, so all messages will be committed in Kafka and consumed by Spark in the same order\n",
    "- There is one leader broker with one replica, so any committed messages will not be lost if the leader broker fails\n",
    "- When creating the topic, the action will only execute if the topic does not already exist\n",
    "- This new topic will be connected to zookeeper and its client port\n",
    "\n",
    "#### To know which broker is doing what, using the \"describe topics\" command of the topic just created\n",
    "`docker-compose exec kafka kafka-topics --describe --topic assess --zookeeper zookeeper:32181`\n",
    "- The output confirms the above statement, and the `isr` value shows that the one replica of our only partition is \"in-sync\" with the leader.\n",
    "\n",
    "#### Check out the individual messages in the json file\n",
    "`docker-compose exec mids bash -c \"cat /w205/project-2-sophiaski/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c\"`\n",
    "\n",
    "#### Check out the message count in the json file: 3280\n",
    "`docker-compose exec mids bash -c \"cat /w205/project-2-sophiaski/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | wc -l\"`\n",
    "\n",
    "#### From the MIDS container, use \"kafkacat\" in producer mode to read the 3280 messages from standard input, specifying the Kafka broker and \"assess\" topic.\n",
    "`docker-compose exec mids bash -c \"cat /w205/project-2-sophiaski/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t assess && echo 'Produced 3280 messages.'\"`\n",
    "\n",
    "\n",
    "#### From the Kafka container, use \"kafka-console-consumer\" tool to print out 10 messages from the specified Kafka broker and \"assess\"  topic, dump them into standard output.\n",
    "`docker-compose exec kafka kafka-console-consumer --bootstrap-server kafka:29092 --topic assess --from-beginning --max-messages 10`\n",
    "\n",
    "#### Spin up a pyspark process in Jupyter Notebook. Open up another tab, and copy and paste URL shared in output, replacing `0.0.0.0` with  external IP from Google Cloud instance.\n",
    "`docker-compose exec spark env PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port 8850 --ip 0.0.0.0 --allow-root --notebook-dir=/w205' pyspark`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the messages with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the json module and pyspark Row method for reading in and loading in the json file.\n",
    "import json\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "+----+--------------------+------+---------+------+--------------------+-------------+\n",
      "| key|               value| topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 6B 65 65 6...|assess|        0|     0|1969-12-31 23:59:...|            0|\n",
      "+----+--------------------+------+---------+------+--------------------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Number of rows: 3280\n"
     ]
    }
   ],
   "source": [
    "# First create a Kafka source in Spark for batch consumption, which subscribes to the \"assess\" topic, at the earliest and latest offsets.\n",
    "raw_assess = spark.read.format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\")\\\n",
    "    .option(\"subscribe\",\"assess\")\\\n",
    "    .option(\"startingOffsets\", \"earliest\")\\\n",
    "    .option(\"endingOffsets\", \"latest\")\\\n",
    "    .load(\"json\")\n",
    "\n",
    "# Cache raw_assess to cut back on warnings later\n",
    "raw_assess.cache()\n",
    "\n",
    "# See the schema, messages, and total count\n",
    "raw_assess.printSchema()\n",
    "raw_assess.show(1)\n",
    "print(\"Number of rows:\",raw_assess.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let's extract the data, promote data columns into real dataframe columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- base_exam_id: string (nullable = true)\n",
      " |-- certification: string (nullable = true)\n",
      " |-- exam_name: string (nullable = true)\n",
      " |-- keen_created_at: string (nullable = true)\n",
      " |-- keen_id: string (nullable = true)\n",
      " |-- keen_timestamp: string (nullable = true)\n",
      " |-- max_attempts: string (nullable = true)\n",
      " |-- sequences: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: array (valueContainsNull = true)\n",
      " |    |    |-- element: map (containsNull = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: boolean (valueContainsNull = true)\n",
      " |-- started_at: string (nullable = true)\n",
      " |-- user_exam_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unrolling json and save to a dataframe. A DataFrame can be created for a JSON dataset represented by\n",
    "# an RDD[String] storing one JSON object per string.\n",
    "assessDF = raw_assess.rdd.map(lambda x: Row(**json.loads(x.value))).toDF()\n",
    "\n",
    "# See the schema\n",
    "assessDF.printSchema()\n",
    "\n",
    "# You can see in the schema that the column \"sequences\" contains nested objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(base_exam_id='37f0a30a-7464-11e6-aa92-a8667f27e5dc', certification='false', exam_name='Normal Forms and All That Jazz Master Class', keen_created_at='1516717442.735266', keen_id='5a6745820eb8ab00016be1f1', keen_timestamp='1516717442.735266', max_attempts='1.0', sequences={'questions': [{'options': None, 'user_correct': False, 'user_incomplete': True, 'id': None, 'user_result': None, 'user_submitted': True}, {'options': None, 'user_correct': False, 'user_incomplete': False, 'id': None, 'user_result': None, 'user_submitted': True}, {'options': None, 'user_correct': True, 'user_incomplete': False, 'id': None, 'user_result': None, 'user_submitted': True}, {'options': None, 'user_correct': True, 'user_incomplete': False, 'id': None, 'user_result': None, 'user_submitted': True}], 'id': None, 'attempt': None, 'counts': None}, started_at='2018-01-23T14:23:19.082Z', user_exam_id='6d4089e4-bde5-4a22-b65f-18bce9ab79c8')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What does the first message look like?\n",
    "assessDF.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"keen_timestamp\": \"1516717442.735266\", \"max_attempts\": \"1.0\", \"started_at\": \"2018-01-23T14:23:19.082Z\", \"base_exam_id\": \"37f0a30a-7464-11e6-aa92-a8667f27e5dc\", \"user_exam_id\": \"6d4089e4-bde5-4a22-b65f-18bce9ab79c8\", \"sequences\": {\"questions\": [{\"user_incomplete\": true, \"user_correct\": false, \"options\": [{\"checked\": true, \"at\": \"2018-01-23T14:23:24.670Z\", \"id\": \"49c574b4-5c82-4ffd-9bd1-c3358faf850d\", \"submitted\": 1, \"correct\": true}, {\"checked\": true, \"at\": \"2018-01-23T14:23:25.914Z\", \"id\": \"f2528210-35c3-4320-acf3-9056567ea19f\", \"submitted\": 1, \"correct\": true}, {\"checked\": false, \"correct\": true, \"id\": \"d1bf026f-554f-4543-bdd2-54dcf105b826\"}], \"user_submitted\": true, \"id\": \"7a2ed6d3-f492-49b3-b8aa-d080a8aad986\", \"user_result\": \"missed_some\"}, {\"user_incomplete\": false, \"user_correct\": false, \"options\": [{\"checked\": true, \"at\": \"2018-01-23T14:23:30.116Z\", \"id\": \"a35d0e80-8c49-415d-b8cb-c21a02627e2b\", \"submitted\": 1}, {\"checked\": false, \"correct\": true, \"id\": \"bccd6e2e-2cef-4c72-8bfa-317db0ac48bb\"}, {\"checked\": true, \"at\": \"2018-01-23T14:23:41.791Z\", \"id\": \"7e0b639a-2ef8-4604-b7eb-5018bd81a91b\", \"submitted\": 1, \"correct\": true}], \"user_submitted\": true, \"id\": \"bbed4358-999d-4462-9596-bad5173a6ecb\", \"user_result\": \"incorrect\"}, {\"user_incomplete\": false, \"user_correct\": true, \"options\": [{\"checked\": false, \"at\": \"2018-01-23T14:23:52.510Z\", \"id\": \"a9333679-de9d-41ff-bb3d-b239d6b95732\"}, {\"checked\": false, \"id\": \"85795acc-b4b1-4510-bd6e-41648a3553c9\"}, {\"checked\": true, \"at\": \"2018-01-23T14:23:54.223Z\", \"id\": \"c185ecdb-48fb-4edb-ae4e-0204ac7a0909\", \"submitted\": 1, \"correct\": true}, {\"checked\": true, \"at\": \"2018-01-23T14:23:53.862Z\", \"id\": \"77a66c83-d001-45cd-9a5a-6bba8eb7389e\", \"submitted\": 1, \"correct\": true}], \"user_submitted\": true, \"id\": \"e6ad8644-96b1-4617-b37b-a263dded202c\", \"user_result\": \"correct\"}, {\"user_incomplete\": false, \"user_correct\": true, \"options\": [{\"checked\": false, \"id\": \"59b9fc4b-f239-4850-b1f9-912d1fd3ca13\"}, {\"checked\": false, \"id\": \"2c29e8e8-d4a8-406e-9cdf-de28ec5890fe\"}, {\"checked\": false, \"id\": \"62feee6e-9b76-4123-bd9e-c0b35126b1f1\"}, {\"checked\": true, \"at\": \"2018-01-23T14:24:00.807Z\", \"id\": \"7f13df9c-fcbe-4424-914f-2206f106765c\", \"submitted\": 1, \"correct\": true}], \"user_submitted\": true, \"id\": \"95194331-ac43-454e-83de-ea8913067055\", \"user_result\": \"correct\"}], \"attempt\": 1, \"id\": \"5b28a462-7a3b-42e0-b508-09f3906d1703\", \"counts\": {\"incomplete\": 1, \"submitted\": 4, \"incorrect\": 1, \"all_correct\": false, \"correct\": 2, \"total\": 4, \"unanswered\": 0}}, \"keen_created_at\": \"1516717442.735266\", \"certification\": \"false\", \"keen_id\": \"5a6745820eb8ab00016be1f1\", \"exam_name\": \"Normal Forms and All That Jazz Master Class\"}\n"
     ]
    }
   ],
   "source": [
    "# For comparison, what does the original first message look like?\n",
    "with open('assessment-attempts-20180128-121051-nested.json', 'r') as f:\n",
    "    df = json.load(f)\n",
    "print(json.dumps(df[0])) # Turn json object into encoded string for printed output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note on \"sequences\" and the nested JSON objects within the encoded messages\n",
    "\n",
    "Using: `raw_assess.rdd.map(lambda x: Row(**json.loads(x.value))).toDF()`\n",
    "\n",
    "This map function passes one row at a time to a lambda function which then applies json.loads on the row of the dataframe, which is JSON text. Json.loads converts the text into key-value pairs in a dictionary. The RDD map transformation converts the data structure to a spark DataFrame.\n",
    "\n",
    "As a result of this transformation, the key-value pairs embedded in the nested `sequences` dictionary is not properly decoded. The results show null values for everything in the `sequences` column except for subset of the questions that have Boolean values: `user_complete`, `user_incomplete`, `user_submitted`, `user_result`.\n",
    "\n",
    "Specifically, the value for `questions` within `sequences` is an array, and so is the value for `options` within `questions`. In sum, the map datatype is incompatible with nested arrays, and to extract this information properly, there would need to be an adjustment to how JSON loads is called into the map function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the Data with SparkSQL\n",
    "\n",
    "The sql function on a SparkSession enables applications to run SQL queries programmatically and returns the result as a DataFrame. Here are some example questions that would be useful for data scientists to know about the streamed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Register the \"assessDF\" DataFrame as a SQL temporary view\n",
    "assessDF.registerTempTable('assess')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. How many assessments are in the dataset?\n",
    "\n",
    "In total, there were 3280 assessments recorded in the dataset, counting by individual messages streamed via Kaffa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|total_assessments|\n",
      "+-----------------+\n",
      "|             3280|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of assessments\n",
    "spark.sql(\"select count(*) as total_assessments from assess\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. How many users took only one assessment?\n",
    "\n",
    "There are 3222 instances of `user_exam_id` that took an assessment once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|only_one_assessment|\n",
      "+-------------------+\n",
      "|               3222|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of people that took only one assessment\n",
    "spark.sql(\"select count(distinct user_exam_id) as only_one_assessment from (select user_exam_id, count(*) as counts from assess group by user_exam_id) where counts = 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Were there individuals that retook an exam?\n",
    "\n",
    "There are 20 instances of `user_exam_id` that repeated an exam, totalling 58 assessments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+-------------------------------------------------------+------+\n",
      "|user_exam_id                        |exam_name                                              |counts|\n",
      "+------------------------------------+-------------------------------------------------------+------+\n",
      "|d4ab4aeb-1368-4866-bc5e-7eee69fd1608|An Introduction to d3.js: From Scattered to Scatterplot|3     |\n",
      "|c320d47f-60d4-49a5-9d6c-67e947979bf0|Beginning C# Programming                               |3     |\n",
      "|00745aef-f3af-4127-855c-afc3b6ef4011|Beginning C# Programming                               |3     |\n",
      "|37cf5b0c-4807-4214-8426-fb1731b57700|Beginning C# Programming                               |3     |\n",
      "|a244c11a-d890-4e3e-893d-d17c5ce2ad05|Beginning C# Programming                               |3     |\n",
      "|6132da16-2c0c-436c-9c48-43b8bafe0978|Beginning C# Programming                               |3     |\n",
      "|66d91177-c436-4ee1-b0b0-daa960e1b2d0|Beginning C# Programming                               |3     |\n",
      "|028ad26f-a89f-4a63-95d4-b6b58f6fa30d|Intermediate C# Programming                            |3     |\n",
      "|949aa36c-74c7-4fc1-a41f-42386c1beb37|Intermediate C# Programming                            |3     |\n",
      "|6e4889ab-5978-44b9-832f-6243300e401f|Intermediate Python Programming                        |2     |\n",
      "|c1eb4d4a-d6ef-43ee-9ef4-58bc6c1d17ff|Intermediate Python Programming                        |2     |\n",
      "|b7ac6d15-97e1-4e94-a09d-da819024b8cd|Introduction to Big Data                               |3     |\n",
      "|a45b5ee6-a4ed-4b18-b962-15abddd765d7|Learning C# Best Practices                             |3     |\n",
      "|3d63ec69-8d97-4f99-82aa-b0786ef21679|Learning C# Best Practices                             |3     |\n",
      "|ac80a11a-2e79-40ef-a756-7edb6f0ddf0b|Learning C# Best Practices                             |3     |\n",
      "|fa23b287-0d0a-4683-8d19-38a65b7f57d1|Learning C# Design Patterns                            |3     |\n",
      "|1e325cc1-47a9-4808-8f6b-508b5459ed6d|Learning C# Design Patterns                            |3     |\n",
      "|bd96cfbe-1532-4ba2-a504-7e8a437a5065|Learning DNS                                           |3     |\n",
      "|a7e6fc04-245f-4e3c-9539-e2aac44c0eb8|Learning Git                                           |3     |\n",
      "|cdc5859d-b332-4fb1-aae4-5cacb52cea5f|Learning Git                                           |3     |\n",
      "+------------------------------------+-------------------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display of 20 users that retook the same exam\n",
    "spark.sql(\"select user_exam_id, exam_name, counts from (select user_exam_id, exam_name, count(*) as counts from assess group by user_exam_id, exam_name) where counts > 1 order by exam_name\").show(assessDF.count(),False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What's the name of your Kafka topic? How did you come up with that name?\n",
    "\n",
    "\"Assess\" was chosen for its readability and ease of understanding. The name of the Kafka topic is called \"assess\" because it is shorthand for \"assessments.\" It is semantically linked to the content of the data, which are individual assessments. It also has a simple structure: all lowercased without special characters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. How many people took \"Learning Git\"?\n",
    "\n",
    "390 individuals took the \"Learning Git\" course. There were 394 total assessments because 2 individuals took the assessment 3 times, which added an extra 4 counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|distinct_learning_git|\n",
      "+---------------------+\n",
      "|390                  |\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count distinct user_exam_id for \"Learning Git\"\n",
    "spark.sql(\"select count(distinct user_exam_id) as distinct_learning_git from (select exam_name, user_exam_id, count(*) as counts from assess where exam_name='Learning Git' group by exam_name, user_exam_id)\").show(assessDF.count(),False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. What is the least common course taken? And the most common?\n",
    "\n",
    "The least common courses were those that were only taken once: which were `Native Web Apps for Android`, `Learning to Visualize Data with D3.js`, `Nulls, Three-valued Logic and Missing Information`, `Operating Red Hat Enterprise Linux Servers`.\n",
    "\n",
    "The most common course was `Learning Git`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+------+\n",
      "|exam_name                                        |counts|\n",
      "+-------------------------------------------------+------+\n",
      "|Native Web Apps for Android                      |1     |\n",
      "|Learning to Visualize Data with D3.js            |1     |\n",
      "|Nulls, Three-valued Logic and Missing Information|1     |\n",
      "|Operating Red Hat Enterprise Linux Servers       |1     |\n",
      "+-------------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Least common courses\n",
    "spark.sql(\"select exam_name, count(*) as counts from assess group by exam_name order by counts asc limit 4\").show(assessDF.count(),False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|exam_name   |counts|\n",
      "+------------+------+\n",
      "|Learning Git|394   |\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Most common course\n",
    "spark.sql(\"select exam_name, count(*) as counts from assess group by exam_name order by counts desc limit 1\").show(assessDF.count(),False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Is there any test data in this dataset?\n",
    "\n",
    "Yes, there are 5 instances where `base_exam_id = example-id` and  `exam_name = Example Exam For Development and Testing oh yeahsdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------------------------------------------+------+\n",
      "|base_exam_id|exam_name                                          |counts|\n",
      "+------------+---------------------------------------------------+------+\n",
      "|example-id  |Example Exam For Development and Testing oh yeahsdf|5     |\n",
      "+------------+---------------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select base_exam_id, exam_name, count(*) as counts from assess where base_exam_id='example-id' group by base_exam_id, exam_name\").show(assessDF.count(),False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. What is the most popular hour to start exams?\n",
    "\n",
    "The most popular time to start an assessment was between 2:00-2:59PM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+\n",
      "|hour|assess_count|\n",
      "+----+------------+\n",
      "|14  |224         |\n",
      "+----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import hour\n",
    "assessDF.withColumn(\"hour\", hour(\"started_at\")).registerTempTable('assessTIME')\n",
    "spark.sql(\"select hour, count(*) as assess_count from assessTIME group by hour order by assess_count desc limit 1\").show(assessDF.count(),False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store assessments in HDFS\n",
    "\n",
    "We will now save the contents of the SparkDataFrame as a Parquet file, preserving the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cast as strings\n",
    "assess_strings = raw_assess.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "# See the schema, messages, and total count\n",
    "assess_strings.printSchema()\n",
    "\n",
    "# Write this to hdfs\n",
    "assess_strings.write.parquet(\"/tmp/assessments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check out results (in another terminal window)\n",
    "`docker-compose exec cloudera hadoop fs -ls /tmp/\n",
    "docker-compose exec cloudera hadoop fs -ls -h /tmp/assessments/`\n",
    "\n",
    "#### Spin down the cluster\n",
    "`docker-compose down`\n",
    "\n",
    "#### Write terminal history to file\n",
    "`history > sophiaski-history.txt`"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
