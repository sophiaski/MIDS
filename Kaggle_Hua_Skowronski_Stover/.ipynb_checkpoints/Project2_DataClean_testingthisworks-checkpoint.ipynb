{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Cleaning the ASHRAE datasets\n",
    "\n",
    "##  Dealing with missing data\n",
    "by: Alissa Stover, Sophia Skowronski, Ying Hua"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'missingno'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ffaa0f912c75>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpsutil\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmissingno\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmsno\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;34m''' For ML'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'missingno'"
     ]
    }
   ],
   "source": [
    "''' importing basic data analysis packages'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import os,random, math, psutil, pickle \n",
    "import missingno as msno\n",
    "\n",
    "''' For ML'''\n",
    "from sklearn import metrics, svm\n",
    "from sklearn.linear_model  import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# PART 1: Imputing missing data in training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Reading in data'''\n",
    "building_df = pd.read_csv('building_metadata.csv')\n",
    "train_df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Initial filtering and merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only focusing on electricity meter:\n",
    "train_df= train_df.loc[train_df['meter']==0]\n",
    "\n",
    "# merging with buidling data\n",
    "train_df = train_df.merge(building_df, on=['building_id'], how='left')\n",
    "\n",
    "# Converting timestamp to right data type\n",
    "train_df['timestamp'] = pd.to_datetime(train_df['timestamp'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add `time_index`, `day_of_week`, and `hour_of_day` variables to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding time index varialbe that counts difference in time vs. beginning date \n",
    "train_df['time_index']= train_df['timestamp']- train_df['timestamp'].min()\n",
    "\n",
    "# Coverting time difference into hours\n",
    "train_df['time_index']= train_df['time_index'].apply (lambda x: x.days*24+x.seconds//3600) \n",
    "train_df['time_index'] = train_df ['time_index'].astype(int)\n",
    "\n",
    "# Adding day of the week and hour of day\n",
    "train_df['day_of_week'] = train_df['timestamp'].dt.dayofweek\n",
    "train_df['hour_of_day'] = train_df['time_index'] % 24\n",
    "train_df['hour_of_day'] = train_df['hour_of_day'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a look at missing data - Explanatory variable `meter_reading`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining how much data is missing\n",
    "missing_count = len(train_df.loc[train_df['meter_reading'].isnull()])\n",
    "missing_percent = missing_count / len(train_df)\n",
    "print (\"{0:.0%}\".format(missing_percent), \"of data is missing.\")\n",
    "\n",
    "# Exmaing how much data has 0 reading\n",
    "zero_value = len(train_df.loc[train_df['meter_reading'] ==0])\n",
    "zero_percent = zero_value / len(train_df)\n",
    "print (\"{0:.0%}\".format(zero_percent), \"of meter readings are 0. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We note that 4% of the meter readings are zero.  We theorize that some of the 0 values are not actual meter readings but some sort of erroneous mistake.  \n",
    "\n",
    "> One of the things we have noticed is that sometimes a 0 reading will be followed by a huge reading and we suspect those are \"catch up\" reading when one misses a meter reading.  We note that while 4% is not a big number (especially considering some of these data could actually be 0 readings, we decided that we should take two approaches.  One is the simple appraoch to get rid of these 0 readings and treat all as mistakes.  The other is to use ML techniques to impute these missing number. \n",
    "\n",
    "> We plan on using both set when we come up with the model to forecast meter reading and see which set perform better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The simple approach - treating all 0 readings as erroneous and deleting all\n",
    "\n",
    "To more accurately impute missing data, we first need a set of clean meter readings that will be representative (i.e. not erroneous.)  We begin our process by hilighting which data could be erroneous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding index as a column \n",
    "train_df['index'] = train_df.index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data quality purpose, we decided to also delete the spike reading immediately post the 0 readings\n",
    "\n",
    "# we first define spike reading to be readings that are >2x standard deviation away from mean for each buildign\n",
    "\n",
    "building_meter_avg = pd.DataFrame(train_df.groupby('building_id')['meter_reading'].mean())\n",
    "building_meter_std = pd.DataFrame(train_df.groupby('building_id')['meter_reading'].std())\n",
    "building_meter_outlier = building_meter_avg.merge(building_meter_std, on='building_id')\n",
    "building_meter_outlier= building_meter_outlier.rename(columns={\"meter_reading_x\": \"avg\", \"meter_reading_y\": \"std\"})\n",
    "building_meter_outlier['outlier'] = building_meter_outlier['avg']+2*building_meter_outlier['std']\n",
    "\n",
    "# Adding the outlier into the original dataframe\n",
    "train_df= train_df.merge(building_meter_outlier, on='building_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determing spike readings that are immediately followed by 0 readings \n",
    "\n",
    "spike_index = []\n",
    "grouped = train_df.groupby('building_id')\n",
    "for key, group in grouped:\n",
    "    group.sort_values(by = 'time_index')\n",
    "    spike_index += list(group.loc[((group.meter_reading > group.outlier) & \\\n",
    "                                   (group.meter_reading.shift(1) == 0))].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering out 0s and spike readings\n",
    "clean_data_naive = train_df.loc[(train_df.meter_reading != 0 & \\\n",
    "                                 ~train_df.index.isin(spike_index))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print ('With this simple way, we are capturing', \"{0:.0%}\".format(len(clean_data_naive)/ \\\n",
    "                                                                 len(train_df)), 'of the meter reading data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_naive.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The advanced approach \n",
    "> With this approach, we want to come up with a ML model to impute missing meter reading data.  To do so, we first need to distinguish which among the 0 readings are missing data and which are actual 0 readings.  We define such as 0 readings that are not followed by spikes and does not last longer than 7 consecutive days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first separate out all the zero readings as well as spikes, which is the naive clean dataset we calcualted above. \n",
    "# since we suspect these spikes are erroneous, we will put them in the to impute category\n",
    "clean_data_advanced = clean_data_naive\n",
    "data_to_impute = train_df.loc[~train_df.index.isin (clean_data_advanced.index)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then add back 0 values that are likely to be real zero readings- i.e. those that are not followed by spikes and does not last 7 consecutive days\n",
    "\n",
    "grouped = train_df.groupby('building_id')\n",
    "correct_zero_readings_index = []\n",
    "for key, group in grouped:\n",
    "    group = group.sort_values(by = 'time_index')\n",
    "    correct_zero_readings_index += list(group.loc[(((group.meter_reading == 0) & \\\n",
    "                                                   (group.meter_reading.shift(-1)<=group.outlier)) & \\\n",
    "                                                   (group.meter_reading.groupby((group.meter_reading != \\\n",
    "                                                                                 group.meter_reading.shift()).cumsum()).\\\n",
    "                                                                                 transform('count').lt(8)))].index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add these back to our clean data and exclude them from data to impute\n",
    "clean_data_advanced = train_df.loc[((train_df.index.isin(list(clean_data_advanced.index))) | \\\n",
    "                                  (train_df.index.isin(correct_zero_readings_index)))]\n",
    "data_to_impute = data_to_impute.drop (correct_zero_readings_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From our visual charts, we can also see that in site 0, there are a large chunks of data that are missing at the beginning of the period.  There are some small readings here and there during this period at certain buildings but for the most part, the readings were non existent. We decided not to include these data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figuring out which part of the site 0 history to ignore\n",
    "site_zero_issues = train_df.loc[train_df.site_id == 0].groupby('time_index')['meter_reading'].sum()\n",
    "max_dff = (site_zero_issues - site_zero_issues.shift()).max()\n",
    "inflection_time = site_zero_issues[site_zero_issues >=  max_dff].index[0]\n",
    "size_zero_missing_data_index = list(clean_data_advanced.loc[((clean_data_advanced.site_id ==0) & \\\n",
    "                                                  (clean_data_advanced.time_index < inflection_time))].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moving these data from clean_data_advanced to data_to_impute\n",
    "clean_data_advanced = clean_data_advanced.drop (size_zero_missing_data_index)\n",
    "data_to_impute =  train_df.loc[((train_df.index.isin(list(data_to_impute.index))) | \\\n",
    "                                  (train_df.index.isin(size_zero_missing_data_index)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We now have a clean dataset (clean_data_advanced) to run ML algorithmn to impute missing data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('We are using', \"{0:.0%}\".format(len(clean_data_advanced)/ len(train_df)), \\\n",
    "       'of the meter reading data as clean data to run ML to impute', \\\n",
    "       \"{0:0}\".format(len(data_to_impute)), ', or', \"{0:.0%}\".format(len(data_to_impute)/ len(train_df)), \"of the data.\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ML to impute missing data\n",
    "\n",
    "We decided to try different ML methods to impute missing data (in our case 0 readings).  The three mehtods we want to try is KNN, linear regression and naive bayes. \n",
    "\n",
    "Before we run different methodology and compare results, we first want to split our clean data into training and test set and define features that will be used to run the test. Since meter_reading is our explanatory variable that we want to use more features later to predict, we want to keep this part of feature engineering simpler.  \n",
    "\n",
    "We picked 4 variables as features - site id, buidling id, time of the day and day of the week. The first 2 variables we think will indirectly give us some information about buildng specific as well as weather related inforamtion as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_advanced_x = clean_data_advanced.loc[:,clean_data_advanced.columns.isin(['building_id', 'site_id','hour_of_day', 'day_of_week'])]\n",
    "clean_data_advanced_y = clean_data_advanced.loc[:,clean_data_advanced.columns.isin(['meter_reading'])].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting & encoding lable to avoid valueerror\n",
    "# lab_enc = preprocessing.LabelEncoder()\n",
    "# training_scores_encoded = lab_enc.fit_transform(clean_data_advanced_y)\n",
    "training_scores_encoded= np.ravel(clean_data_advanced_y).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting the clean dataset into 70/30 for training/ test\n",
    "X_train, X_test, y_train, y_test = train_test_split(clean_data_advanced_x, training_scores_encoded, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> METHOD: Liner regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model using linear regression\n",
    "regressor = LinearRegression()  \n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting y \n",
    "y_pred = regressor.predict(X_test)\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced approach, attempt 2\n",
    "\n",
    "> Given a extremely low success score (especially even on the training set) we suspected that the model is flawed.  One of the issues we see is that we have essentially all categorical variables even though they are in numeric values.  So we decide to experiementing on reconfiguring these variables into more numeric values.  The way we do this is by taking the average that fits each catergory and use that train the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of the x, y variables to test our new method\n",
    "clean_data_advanced_new_x = clean_data_advanced.loc[:,clean_data_advanced.columns.isin(['building_id', 'site_id','meter_reading','hour_of_day', 'day_of_week'])].copy()\n",
    "clean_data_advanced_y = clean_data_advanced.loc[:,clean_data_advanced.columns.isin(['meter_reading'])].copy()\n",
    "clean_data_advanced_y= np.ravel(clean_data_advanced_y).astype('int')\n",
    "\n",
    "# lab_enc = preprocessing.LabelEncoder()\n",
    "# training_scores_encoded = lab_enc.fit_transform(clean_data_advanced_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting the clean dataset into 70/30 for training/ test\n",
    "X_train, X_test, y_train, y_test = train_test_split(clean_data_advanced_new_x, clean_data_advanced_y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding in avearage meter reading for each variable for training_set\n",
    "X_train = X_train.copy()\n",
    "X_train ['avg_building'] = X_train.groupby('building_id')['meter_reading'].transform('mean')\n",
    "X_train ['avg_site'] = X_train.groupby('site_id')['meter_reading'].transform('mean')\n",
    "X_train ['avg_dow'] = X_train.groupby('day_of_week')['meter_reading'].transform('mean')\n",
    "X_train ['avg_hod'] = X_train.groupby('hour_of_day')['meter_reading'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dictionary\n",
    "\n",
    "grouped = X_train.groupby('building_id')\n",
    "avg_building_dict= {key:group['avg_building'].mean() for key, group in grouped}\n",
    "\n",
    "grouped = X_train.groupby('site_id')\n",
    "avg_site_dict= {key:group['avg_site'].mean() for key, group in grouped}\n",
    "\n",
    "grouped = X_train.groupby('day_of_week')\n",
    "avg_dow_dict= {key:group['avg_dow'].mean() for key, group in grouped}\n",
    "\n",
    "grouped = X_train.groupby('hour_of_day')\n",
    "avg_hod_dict= {key:group['avg_hod'].mean() for key, group in grouped}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding in avearage meter reading for each variable for testing_set\n",
    "X_test = X_test.copy()\n",
    "X_test ['avg_building'] = X_test['building_id'].map(avg_building_dict)\n",
    "X_test ['avg_site'] = X_test['site_id'].map(avg_site_dict)\n",
    "X_test ['avg_dow'] = X_test['day_of_week'].map(avg_dow_dict)\n",
    "X_test ['avg_hod'] = X_test['hour_of_day'].map(avg_hod_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping categorical columns\n",
    "X_train = X_train.drop(columns =['building_id','meter_reading','site_id','day_of_week', 'hour_of_day'])\n",
    "X_test = X_test.drop(columns=['building_id','meter_reading','site_id','day_of_week', 'hour_of_day'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **METHOD: Linear_ regression- ATTEMPT 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model using linear regression\n",
    "regressor = LinearRegression()  \n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting y \n",
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating results \n",
    "df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using linear regression to impute missing value\n",
    "> With a 87% R-square, we feel pretty good about our model and will go ahead and use the model to forecast the missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_impute.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_impute_cal = data_to_impute.loc[:,clean_data_advanced.columns.isin(['building_id', 'site_id','hour_of_day', 'day_of_week'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translating categorical data\n",
    "data_to_impute_cal = data_to_impute_cal.copy()\n",
    "data_to_impute_cal ['avg_building'] = data_to_impute_cal['building_id'].map(avg_building_dict)\n",
    "data_to_impute_cal ['avg_site'] = data_to_impute_cal['site_id'].map(avg_site_dict)\n",
    "data_to_impute_cal ['avg_dow'] = data_to_impute_cal['day_of_week'].map(avg_dow_dict)\n",
    "data_to_impute_cal ['avg_hod'] = data_to_impute_cal['hour_of_day'].map(avg_hod_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping categorical columns\n",
    "data_to_impute_cal = data_to_impute_cal.drop(columns =['building_id','site_id','day_of_week', 'hour_of_day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting data_to_impute\n",
    "data_to_impute_cal ['meter_reading'] = regressor.predict(data_to_impute_cal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_meter_reading = pd.DataFrame (clean_data_advanced['meter_reading'].append(data_to_impute_cal['meter_reading']))\n",
    "full_meter_reading = full_meter_reading.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_imputed = train_df.copy()\n",
    "train_df_imputed['meter_reading'] = full_meter_reading['meter_reading']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a look at missing data - Independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = pd.DataFrame(train_df_imputed.isna().sum())\n",
    "missing_data.columns = ['missing_count']\n",
    "missing_data['missing_percent'] = missing_data['missing_count']/ len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  The two missing data are year_built and floor_count. We note that both of them have fairly high percent of missing data.  While we will use ML techniques to impute these missing data, we will be more inclined to use other non-missing data if possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing year_built variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_imputed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Are there any buildings that are not missing year_built consistently?\n",
    "grouped = train_df_imputed.groupby('building_id')\n",
    "inconsistent_index = []\n",
    "for key, group in grouped:\n",
    "    if group['year_built'].isna().count() not in [len(group), 0]:\n",
    "        inconsistent_index += [key]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We confirm no building is missing partial data\n",
    "len(inconsistent_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we then separate the missing data form non-missing data \n",
    "yb_missing_data = train_df_imputed.loc[train_df_imputed['year_built'].isna()]\n",
    "yb_clean_data = train_df_imputed.loc[~train_df_imputed['year_built'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For ML algorithm, we will split the non-missing data to 80/20 for train/test.  We will pick the followingvariable as features: meter_reading, day_of_week, hour_of_day, square_feet, primary_use, site_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting x-variables \n",
    "yb_clean_data_x = yb_clean_data.loc[:, yb_clean_data.columns.isin(['meter_reading', 'day_of_week', 'hour_of_day',\\\n",
    "                                                  'squre_feet', 'primary_use', 'site_id'])]\n",
    "# Setting y-variables\n",
    "yb_clean_data_y = yb_clean_data.loc[:, yb_clean_data.columns.isin(['year_built'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting primary_use into a numeric varialbe\n",
    "lab_enc = preprocessing.LabelEncoder()\n",
    "yb_clean_data_x = yb_clean_data_x.copy()\n",
    "yb_clean_data_x['primary_use'] = lab_enc.fit_transform(yb_clean_data_x['primary_use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting dataset into 70/30 for train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(yb_clean_data_x, yb_clean_data_y, test_size=0.3, random_state=1)\n",
    "y_train = np.ravel(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ML Method: KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "# Fit the classifier to the data\n",
    "knn.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on test data\n",
    "y_predict = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check accuracy of our model on the test data\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model using linear regression\n",
    "regressor = LinearRegression()  \n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting y \n",
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "# Train classifier\n",
    "gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict values\n",
    "\n",
    "y_pred = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Of these methods, KNN gave us the best results.  We now explore accuracy with different n_neighbor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 15)\n",
    "# Fit the classifier to the data\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "# Predicting on test data\n",
    "y_predict = knn.predict(X_test)\n",
    "\n",
    "#check accuracy of our model on the test data\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We think using KNN and 15 n-neighbor values gives us highest  accuracy. We will use this to impute missing year_built data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predicting year_built data in  missing_data \n",
    "yb_missing_data_x= yb_missing_data.loc[:,yb_missing_data.columns.isin(['meter_reading', 'day_of_week', 'hour_of_day',\\\n",
    "                                                  'squre_feet', 'primary_use', 'site_id'])]\n",
    "yb_missing_data_x = yb_missing_data_x.copy()\n",
    "yb_missing_data_x['primary_use'] = lab_enc.fit_transform(yb_missing_data_x['primary_use'])\n",
    "yb_missing_data = yb_missing_data.copy()\n",
    "yb_missing_data ['year_built'] = knn.predict(yb_missing_data_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_yb = pd.DataFrame (yb_clean_data['year_built'].append(yb_missing_data['year_built']))\n",
    "full_yb = full_yb.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_imputed = train_df_imputed.copy()\n",
    "train_df_imputed['year_built'] = full_yb['year_built']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing floor_count variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we then separate the missing data form non-missing data \n",
    "fc_missing_data = train_df_imputed.loc[train_df_imputed['floor_count'].isna()]\n",
    "fc_clean_data = train_df_imputed.loc[~train_df_imputed['floor_count'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For ML algorithm, we will split the non-missing data to 70/30 for train/test.  We will pick the followingvariable as features: meter_reading, day_of_week, hour_of_day, square_feet, primary_use, site_id, year_built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting x-variables \n",
    "fc_clean_data_x = fc_clean_data.loc[:, fc_clean_data.columns.isin(['meter_reading', 'day_of_week', 'hour_of_day',\\\n",
    "                                                  'squre_feet', 'primary_use', 'site_id', 'year_built'])]\n",
    "# Setting y-variables\n",
    "fc_clean_data_y = fc_clean_data.loc[:, fc_clean_data.columns.isin(['floor_count'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting primary_use into a numeric varialbe\n",
    "lab_enc = preprocessing.LabelEncoder()\n",
    "fc_clean_data_x = fc_clean_data_x.copy()\n",
    "fc_clean_data_x['primary_use'] = lab_enc.fit_transform(fc_clean_data_x['primary_use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting dataset into 70/30 for train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(fc_clean_data_x, fc_clean_data_y, test_size=0.3, random_state=1)\n",
    "y_train = np.ravel(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ML Method: KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "# Fit the classifier to the data\n",
    "knn.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on test data\n",
    "y_predict = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check accuracy of our model on the test data\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 87% accuracy is pretty good. we will use this to impute missing floor_count data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting floor_count data in  missing_data \n",
    "fc_missing_data_x= fc_missing_data.loc[:,fc_missing_data.columns.isin(['meter_reading', 'day_of_week', 'hour_of_day',\\\n",
    "                                                  'squre_feet', 'primary_use', 'site_id', 'year_built'])]\n",
    "fc_missing_data_x = fc_missing_data_x.copy()\n",
    "fc_missing_data_x['primary_use'] = lab_enc.fit_transform(fc_missing_data_x['primary_use'])\n",
    "fc_missing_data = fc_missing_data.copy()\n",
    "fc_missing_data ['floor_count'] = knn.predict(fc_missing_data_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_fc = pd.DataFrame (fc_clean_data['floor_count'].append(fc_missing_data['floor_count']))\n",
    "full_fc = full_fc.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_imputed = train_df_imputed.copy()\n",
    "train_df_imputed['floor_count'] = full_fc['floor_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_imputed.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the final data set with imputed missing values for building data and meter reading\n",
    "#train_df_imputed.to_pickle('train_df_imputed.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# PART 2: Daylight savings correction for train & test data\n",
    "\n",
    "This Jupyter notebook walks through steps to read in and reduce the memory usage of the clean meter reading files. \n",
    "It localizes the timezones and adjusts them for daylight savings time, with data derived from the discussion here: https://www.kaggle.com/patrick0302/locate-cities-according-weather-temperature \n",
    "This code also derives from code found at this URL https://www.kaggle.com/caesarlupum/ashrae-ligthgbm-simple-fe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaned train data\n",
    "train_tz_df = train_df_imputed\n",
    "train_tz_df[\"timestamp\"] = pd.to_datetime(train_tz_df[\"timestamp\"], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Test data\n",
    "test_tz_df = pd.read_csv('test.csv')\n",
    "test_tz_df[\"timestamp\"] = pd.to_datetime(test_tz_df[\"timestamp\"], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Building data\n",
    "building_df = pd.read_csv('building_metadata.csv')\n",
    "building_df['primary_use'] = building_df['primary_use'].astype('category')\n",
    "\n",
    "# Timezone data\n",
    "time_zones_df = pd.read_csv('time_zones.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge train/test data with timezone data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge building data on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = test_tz_df[['building_id']]\n",
    "temp_df = temp_df.merge(building_df, on = ['building_id'], how = 'left')\n",
    "\n",
    "del temp_df['building_id']\n",
    "test_tz_df = pd.concat([test_tz_df, temp_df], axis = 1)\n",
    "\n",
    "del temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = train_tz_df[['site_id']]\n",
    "temp_df = temp_df.merge(time_zones_df, on = ['site_id'], how = 'left')\n",
    "\n",
    "del temp_df['site_id']\n",
    "train_tz_df = pd.concat([train_tz_df, temp_df], axis=1)\n",
    "\n",
    "del temp_df\n",
    "\n",
    "temp_df = test_tz_df[['site_id']]\n",
    "temp_df = temp_df.merge(time_zones_df, on = ['site_id'], how = 'left')\n",
    "\n",
    "del temp_df['site_id']\n",
    "test_tz_df = pd.concat([test_tz_df, temp_df], axis=1)\n",
    "\n",
    "del temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tz_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcting Daylight Savings Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare daylight savings time column for adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tz_df['dst'] = 0\n",
    "test_tz_df['dst'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "\n",
    "# 2016\n",
    "\n",
    "train_tz_df.loc[((train_tz_df['timezone'] == 'US/Eastern') & \n",
    "                 (train_tz_df['timestamp'] >= '2016-03-13 02:00:00') & \n",
    "                 (train_tz_df['timestamp'] < '2016-11-06 01:00:00')), 'dst'] = 1\n",
    "train_tz_df.loc[((train_tz_df['timezone'] == 'US/Mountain') & \n",
    "                 (train_tz_df['timestamp'] >= '2016-03-13 02:00:00') & \n",
    "                 (train_tz_df['timestamp'] < '2016-11-06 01:00:00')), 'dst'] = 1\n",
    "train_tz_df.loc[((train_tz_df['timezone'] == 'US/Pacific') & \n",
    "                 (train_tz_df['timestamp'] >= '2016-03-13 02:00:00') & \n",
    "                 (train_tz_df['timestamp'] < '2016-11-06 01:00:00')), 'dst'] = 1\n",
    "train_tz_df.loc[((train_tz_df['timezone'] == 'US/Central') & \n",
    "                 (train_tz_df['timestamp'] >= '2016-03-13 02:00:00') & \n",
    "                 (train_tz_df['timestamp'] < '2016-11-06 01:00:00')), 'dst'] = 1\n",
    "train_tz_df.loc[((train_tz_df['timezone'] == 'Canada/Eastern') & \n",
    "                 (train_tz_df['timestamp'] >= '2016-03-13 02:00:00') & \n",
    "                 (train_tz_df['timestamp'] < '2016-11-06 01:00:00')), 'dst'] = 1\n",
    "train_tz_df.loc[((train_tz_df['timezone'] == 'Europe/London') & \n",
    "                 (train_tz_df['timestamp'] >= '2016-03-27 01:00:00') & \n",
    "                 (train_tz_df['timestamp'] < '2016-10-30 02:00:00')), 'dst'] = 1\n",
    "train_tz_df.loc[((train_tz_df['timezone'] == 'Europe/Dublin') & \n",
    "                 (train_tz_df['timestamp'] >= '2016-03-27 01:00:00') & \n",
    "                 (train_tz_df['timestamp'] < '2016-10-30 02:00:00')), 'dst'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "\n",
    "# 2017\n",
    "\n",
    "test_tz_df.loc[((test_tz_df['timezone'] == 'US/Eastern') & \n",
    "                (test_tz_df['timestamp'] >= '2017-03-12 02:00:00') & \n",
    "                (test_tz_df['timestamp'] < '2017-11-05 01:00:00')), 'dst'] = 1\n",
    "test_tz_df.loc[((test_tz_df['timezone'] == 'US/Mountain') & \n",
    "                (test_tz_df['timestamp'] >= '2017-03-12 02:00:00') & \n",
    "                (test_tz_df['timestamp'] < '2017-11-05 01:00:00')), 'dst'] = 1\n",
    "test_tz_df.loc[((test_tz_df['timezone'] == 'US/Pacific') & \n",
    "                (test_tz_df['timestamp'] >= '2017-03-12 02:00:00') & \n",
    "                (test_tz_df['timestamp'] < '2017-11-05 01:00:00')), 'dst'] = 1\n",
    "test_tz_df.loc[((test_tz_df['timezone'] == 'US/Central') & \n",
    "                (test_tz_df['timestamp'] >= '2017-03-12 02:00:00') & \n",
    "                (test_tz_df['timestamp'] < '2017-11-05 01:00:00')), 'dst'] = 1\n",
    "test_tz_df.loc[((test_tz_df['timezone'] == 'Canada/Eastern') & \n",
    "                (test_tz_df['timestamp'] >= '2017-03-12 02:00:00') & \n",
    "                (test_tz_df['timestamp'] < '2017-11-05 01:00:00')), 'dst'] = 1\n",
    "test_tz_df.loc[((test_tz_df['timezone'] == 'Europe/London') & \n",
    "                (test_tz_df['timestamp'] >= '2017-03-26 01:00:00') & \n",
    "                (test_tz_df['timestamp'] < '2017-10-29 02:00:00')), 'dst'] = 1\n",
    "test_tz_df.loc[((test_tz_df['timezone'] == 'Europe/Dublin') & \n",
    "                (test_tz_df['timestamp'] >= '2017-03-26 01:00:00') & \n",
    "                (test_tz_df['timestamp'] < '2017-10-29 02:00:00')), 'dst'] = 1\n",
    "\n",
    "# 2018\n",
    "\n",
    "test_tz_df.loc[((test_tz_df['timezone'] == 'US/Eastern') & \n",
    "                (test_tz_df['timestamp'] >= '2018-03-11 02:00:00') & \n",
    "                (test_tz_df['timestamp'] < '2018-11-04 01:00:00')), 'dst'] = 1\n",
    "test_tz_df.loc[((test_tz_df['timezone'] == 'US/Mountain') & \n",
    "                (test_tz_df['timestamp'] >= '2018-03-11 02:00:00') & \n",
    "                (test_tz_df['timestamp'] < '2018-11-04 01:00:00')), 'dst'] = 1\n",
    "test_tz_df.loc[((test_tz_df['timezone'] == 'US/Pacific') & \n",
    "                (test_tz_df['timestamp'] >= '2018-03-11 02:00:00') & \n",
    "                (test_tz_df['timestamp'] < '2018-11-04 01:00:00')), 'dst'] = 1\n",
    "test_tz_df.loc[((test_tz_df['timezone'] == 'US/Central') & \n",
    "                (test_tz_df['timestamp'] >= '2018-03-11 02:00:00') & \n",
    "                (test_tz_df['timestamp'] < '2018-11-04 01:00:00')), 'dst'] = 1\n",
    "test_tz_df.loc[((test_tz_df['timezone'] == 'Canada/Eastern') & \n",
    "                (test_tz_df['timestamp'] >= '2018-03-11 02:00:00') & \n",
    "                (test_tz_df['timestamp'] < '2018-11-04 01:00:00')), 'dst'] = 1\n",
    "test_tz_df.loc[((test_tz_df['timezone'] == 'Europe/London') & \n",
    "                (test_tz_df['timestamp'] >= '2018-03-25 01:00:00') & \n",
    "                (test_tz_df['timestamp'] < '2018-10-28 02:00:00')), 'dst'] = 1\n",
    "test_tz_df.loc[((test_tz_df['timezone'] == 'Europe/Dublin') & \n",
    "                (test_tz_df['timestamp'] >= '2018-03-25 01:00:00') & \n",
    "                (test_tz_df['timestamp'] < '2018-10-28 02:00:00')), 'dst'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust for daylight savings time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta \n",
    "train_tz_df.loc[train_tz_df['dst'] == 1, 'timestamp'] += timedelta(hours = 1)\n",
    "test_tz_df.loc[test_tz_df['dst'] == 1, 'timestamp'] += timedelta(hours = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Minification\n",
    "\n",
    "Save the final dataframes as pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_tz_df.to_pickle('train_imputed_tz_df.pkl')\n",
    "#test_tz_df.to_pickle('test_imputed_tz_df.pkl')\n",
    "#del train_tz_df, test_tz_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the files\n",
    "To use these files, you must first read them in using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_tz_df = pd.read_pickle('train_imputed_tz_df.pkl')\n",
    "#test_tz_df = pd.read_pickle('test_imputed_tz_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# PART 3: Timezone correction for weather train & test data\n",
    "\n",
    "This Jupyter notebook timezone-corrects the weather data; see discussion here for the source of timezone data: https://www.kaggle.com/patrick0302/locate-cities-according-weather-temperature \n",
    "\n",
    "It also derives from code found at this URL https://www.kaggle.com/caesarlupum/ashrae-ligthgbm-simple-fe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "weather_train_tz_df = pd.read_csv('weather_train.csv')\n",
    "\n",
    "# Test data\n",
    "weather_test_tz_df = pd.read_csv ('weather_test.csv')\n",
    "\n",
    "# Timezone data\n",
    "time_zones_df = pd.read_csv('time_zones.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust weather data timezones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge weather & time zone data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "temp_df = weather_train_tz_df[['site_id']]\n",
    "temp_df = temp_df.merge(time_zones_df, on = ['site_id'], how = 'left')\n",
    "\n",
    "del temp_df['site_id']\n",
    "weather_train_tz_df = pd.concat([weather_train_tz_df, temp_df], axis=1)\n",
    "\n",
    "del temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "temp_df = weather_test_tz_df[['site_id']]\n",
    "temp_df = temp_df.merge(time_zones_df, on = ['site_id'], how = 'left')\n",
    "\n",
    "del temp_df['site_id']\n",
    "weather_test_tz_df = pd.concat([weather_test_tz_df, temp_df], axis=1)\n",
    "\n",
    "del temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create timezones dictionary to map timezone offsets onto `timestamp` series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'US/Eastern': -5,\n",
       " 'Europe/London': 0,\n",
       " 'US/Mountain': -7,\n",
       " 'US/Pacific': -8,\n",
       " 'Canada/Eastern': -5,\n",
       " 'US/Central': -6,\n",
       " 'Europe/Dublin': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timezones = list(time_zones_df.timezone.unique())\n",
    "timezones_offset = [-5, 0, -7, -8, -5, -6, 1]\n",
    "timezones_dict = dict(zip(timezones, timezones_offset))\n",
    "timezones_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The \"Unnamed: 0\" is a duplicate `site_id` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_train_tz_df[\"Unnamed: 0\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del weather_train_tz_df[\"Unnamed: 0\"], weather_test_tz_df[\"Unnamed: 0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert `timstamp` series in weather test/train datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "weather_train_tz_df['timestamp'] = pd.to_datetime(weather_train_tz_df['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c3aeae6c9ab2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mweather_test_tz_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'timestamp'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweather_test_tz_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'timestamp'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Test data\n",
    "weather_test_tz_df['timestamp'] = pd.to_datetime(weather_test_tz_df['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `timestamp` adjustment using timezone offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['US/Eastern', 'Europe/London', 'US/Mountain', 'US/Pacific', 'Canada/Eastern', 'US/Central', 'Europe/Dublin'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timezones_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sski\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\sski\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "weather_train_tz_df['timestamp_utc'] = weather_train_tz_df['timestamp']\n",
    "weather_test_tz_df['timestamp_utc'] = weather_test_tz_df['timestamp']\n",
    "\n",
    "from datetime import timedelta \n",
    "for zone in timezones_dict.keys():\n",
    "    weather_train_tz_df.timestamp[weather_train_tz_df.timezone==zone] += timedelta(hours = timezones_dict[zone])\n",
    "    weather_test_tz_df.timestamp[weather_test_tz_df.timezone==zone] += timedelta(hours = timezones_dict[zone])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Minification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weather_train_tz_df.to_pickle('weather_train_tz_df.pkl')\n",
    "#weather_train_tz_df.to_pickle('weather_test_tz_df.pkl')\n",
    "#del weather_train_tz_df, weather_test_tz_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the files\n",
    "To use these files, you must first read them in using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weather_train_tz_df = pd.read_pickle('weather_train_tz_df.pkl')\n",
    "#weather_test_tz_df = pd.read_pickle('weather_test_tz_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# PART 4: Merge imputed and timezone-corrected weather and meter data\n",
    "\n",
    "This Jupyter notebook merges the meter and weather data after they have been timezone-corrected and after the meter data has been cleaned.\n",
    "See discussion here for background on the timezone correction: https://www.kaggle.com/patrick0302/locate-cities-according-weather-temperature \n",
    "This code also derives from code found at this URL https://www.kaggle.com/caesarlupum/ashrae-ligthgbm-simple-fe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meter data\n",
    "#train_tz_df = pd.read_pickle('train_imputed_tz_df.pkl')\n",
    "#test_tz_df = pd.read_pickle('test_imputed_tz_df.pkl')\n",
    "\n",
    "# Weather data\n",
    "#weather_train_tz_df = pd.read_pickle('weather_train_tz_df.pkl')\n",
    "#weather_test_tz_df = pd.read_pickle('weather_test_tz_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather df merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = train_tz_df[['site_id','timestamp', 'timezone', 'country_code', 'location']]\n",
    "temp_df = temp_df.merge(weather_train_tz_df, on=['site_id','timestamp', 'timezone', 'country_code', 'location'], how='left')\n",
    "del temp_df['site_id'], temp_df['timestamp'], temp_df['timezone'], temp_df['country_code'], temp_df['location']\n",
    "\n",
    "train_tz_df = pd.concat([train_tz_df, temp_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = test_tz_df[['site_id','timestamp', 'timezone', 'country_code', 'location']]\n",
    "temp_df = temp_df.merge(weather_test_tz_df, on=['site_id','timestamp', 'timezone', 'country_code', 'location'], how='left')\n",
    "\n",
    "del temp_df['site_id'], temp_df['timestamp'], temp_df['timezone'], temp_df['country_code'], temp_df['location']\n",
    "test_tz_df = pd.concat([test_tz_df, temp_df], axis=1)\n",
    "\n",
    "del temp_df, weather_train_tz_df, weather_test_tz_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_tz_df[\"Unnamed: 0\"], test_tz_df[\"Unnamed: 0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['row_id', 'building_id', 'meter', 'timestamp', 'site_id', 'primary_use',\n",
       "       'square_feet', 'year_built', 'floor_count', 'timezone', 'country_code',\n",
       "       'location', 'timezone_offset', 'dst', 'air_temperature',\n",
       "       'cloud_coverage', 'dew_temperature', 'precip_depth_1_hr',\n",
       "       'sea_level_pressure', 'wind_direction', 'wind_speed', 'timezone_offset',\n",
       "       'timestamp_utc'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tz_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['building_id', 'meter', 'timestamp', 'meter_reading', 'site_id',\n",
       "       'primary_use', 'square_feet', 'year_built', 'floor_count', 'time_index',\n",
       "       'day_of_week', 'hour_of_day', 'index', 'avg', 'std', 'outlier',\n",
       "       'timezone', 'country_code', 'location', 'timezone_offset', 'dst',\n",
       "       'air_temperature', 'cloud_coverage', 'dew_temperature',\n",
       "       'precip_depth_1_hr', 'sea_level_pressure', 'wind_direction',\n",
       "       'wind_speed', 'timezone_offset', 'timestamp_utc', 'air_temperature',\n",
       "       'cloud_coverage', 'dew_temperature', 'precip_depth_1_hr',\n",
       "       'sea_level_pressure', 'wind_direction', 'wind_speed', 'timezone_offset',\n",
       "       'timestamp_utc'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tz_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Minification\n",
    "\n",
    "Save the final dataframes as pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tz_df.to_pickle('train_merge_df.pkl')\n",
    "test_tz_df.to_pickle('test_merge_df.pkl')\n",
    "   \n",
    "del train_tz_df, test_tz_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the files\n",
    "To use these files, you must first read them in using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle('train_merge_df.pkl')\n",
    "test_df = pd.read_pickle('test_merge_df.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
